---
title: "Project1"
author: "Sarba"
date: "11/23/2021"
output:
  word_document: default
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(moderndive)
```

I can import datasets from various sources:

```{r}
# Importing data
data <- read_csv("/home/upretys/STA 518/Project/Project1/data.csv")
```
I imported the dataset using the read_csv function from my GitHub account.


```{r}
# Dropping NAs
data <- data %>%
  drop_na()
```

In almost every dataset, there are missing values. I used the above function to remove all the missing values so that I can work with a tidier dataset.

```{r}
# Calculating mean of each columns without using for loop

# Calculating the average age of participants
mean(data$age)

# Calculating the average elapsed time
mean(data$elapse)

# Calculating the average score
mean(data$score)
```
The mean age of the participants is 34.01423. The average time they took to complete the survey was 119554.2 seconds. The average score of the participants was 13.29965. I used the mean() function to calculate the average of certain columns by extracting those using $ from the dataset.


```{r}
max(data$age)
```
The oldest participant was 509 years old.I used the max function to calculate the maximum age of the participants.

I can tidy datasets and isolate information from the datasets:
```{r}
# Isolating score, elapse, gender, and age from a larger dataset)
df <- data %>%
  select(score,elapse:age) %>%
  mutate(Number = sample(1:11243))
```

The original dataset had a lot of columns that were not very relevant. So, I used the select function to select only the columns that were relevant for my project using the select function. I used the sample() function to randomly assign an ID to the participants, and then used the mutate function to create a new column with the IDs.

```{r} 
# Calculating median of each column using for loop
med <- vector("double", ncol(df))
for (i in sequence(ncol(df))) { 
  med[i] = median(df[[i]]) 
}
med
```
Here, I have used for loop to go through each column in the dataset and compute the median of the values of those columns. In the dataset I am using for this project, there are only five columns so I could have done it without the loop too, but I wanted to demonstrate the idea that if there were infinite columns, this would be the best approach as it reduces the codes from being repetitive and is time-efficient as well. 


```{r}
#Removing Data
#rm(cleaned_data)
#rm(data)
#rm(highest_top10)
#rm(lowest_top10)
#rm(top10)
#rm(df)
```

Tidying the data by replacing values:
```{r}
# Replacing the values "1" and "2" in "Gender" column to "Male" and "Female"
df <- within(df, gender[gender == '1'] <- 'Male')
df <- within(df, gender[gender == '2'] <- 'Female')
df <- within(df, gender[gender == '3'] <- 'Non-Binary')
df <- within(df, gender[gender == '0'] <- 'Not Specified')
```

Previously, the dataset included 1,2,3 and 0 in the 'gender' column which was hard to understand and use to visualize the dataset. So, I used the within function to replace it with values that have meanings and is easy to understand. For example, Male is easier to understand than 1 in the gender column.

```{r}
# Arranging the scores in a descending order to find the top 10 highest scoring age a
highest_top10 <- df %>%
  arrange(desc(score)) %>%
  slice(1:10)
top_n(df, 10, score)

# Arranging the scores in an ascending order to find the top 10 least scoring age
lowest_top10 <- df %>%
  arrange(score) %>%
  slice(1:10)
```
The table above shows the highest scores and who achieved it.


I am capable of creating graphical displays and numerical summaries of data for exploratory analysis and presentations:

```{r}
# Data Visualization to find the total number of people of certain age.
ggplot(data = df) +
  geom_freqpoly(mapping = aes(x = age))

# Data Visualization to find the total number of people who identify with a certain gender.
ggplot(data = df) +
  geom_bar(mapping = aes(x = gender))
```
I have used ggplot to visualize the data. From the above graph, we can see that male participants had the highest count with around 6500 participants, females had the second highest with around 4500 participants, non-binary participants came in third with a count of less than perhaps 50, and participants who had not specified their gender came in last with a count of perhaps 10.


Estimating the proportion of females when a random sampling is drawn:
```{r}
# Random sampling to find out the proportions of females in the dataset

#Taking 50 samples:
females_sample_50 <- df %>% 
  rep_sample_n(size = 50, reps = 1000)

females_proportion_50 <- females_sample_50 %>% 
  group_by(Number) %>% 
  summarize(female = sum(gender == "Female")) %>% 
  mutate(proportion_females = female / 50)

ggplot(females_proportion_50, aes(x = proportion_females)) +
  geom_histogram(binwidth = 0.05, boundary = 0.4, color = "red") +
  labs(x = "Proportion of females", title = "50") 
```

```{r}
#Taking 100 samples:
females_sample_100 <- df %>% 
  rep_sample_n(size = 100, reps = 1000)

females_proportion_100 <- females_sample_100 %>% 
  group_by(Number) %>% 
  summarize(female = sum(gender == "Female")) %>% 
  mutate(proportion_females = female / 100)

ggplot(females_proportion_100, aes(x = proportion_females)) +
  geom_histogram(binwidth = 0.05, boundary = 0.4, color = "red") +
  labs(x = "Proportion of females", title = "100")
```

```{r}
#Taking 1000 samples:

females_sample_1000 <- df %>% 
  rep_sample_n(size = 1000, reps = 1000)

females_proportion_1000 <- females_sample_1000 %>% 
  group_by(Number) %>% 
  summarize(female = sum(gender == "Female")) %>% 
  mutate(proportion_females = female / 1000)

ggplot(females_proportion_1000, aes(x = proportion_females)) +
  geom_histogram(binwidth = 0.05, boundary = 0.4, color = "red") +
  labs(x = "Proportion of females", title = "1000")
```
I have used the same functions for all three simulations above, with the only difference being that I have used different sample sizes for each of those. For the first one, I have used 50, for the second one, I have used 100, and for the third one, I have used 1000. We can see that as the sample size increases, variation decreases. If we look closely, all three histograms seem to be relaying the same message, that the proportion of females between 0.00 and 0.05 is the largest with the count of around 6500. 0.05-0.10 having the second biggest count, and it continues to decrease from there while maintaining almost the same count despite such difference in the sample sizes.
